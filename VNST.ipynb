{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNj1EDztXUyGBzjtLmfYt5W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nasacode1/StyleTransfer/blob/main/VNST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2     # for capturing videos\n",
        "import math   # for mathematical operations\n",
        "import matplotlib.pyplot as plt    # for plotting the images\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "kPTXciT46dB0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install np_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU0rvoMs6jYJ",
        "outputId": "f8e4ba15-8ae0-4e74-d077-df84e6452f6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m617.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from np_utils) (1.23.5)\n",
            "Building wheels for collected packages: np_utils\n",
            "  Building wheel for np_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56439 sha256=0e44cb82766a54bb1112252038057c1ad05a69e8f57d9a332f830265bc5f6c59\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c7/50/2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
            "Successfully built np_utils\n",
            "Installing collected packages: np_utils\n",
            "Successfully installed np_utils-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image   # for preprocessing the images\n",
        "import numpy as np    # for mathematical operations\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras import utils as to_categorical\n",
        "from skimage.transform import resize   # for resizing images\n",
        "import os"
      ],
      "metadata": {
        "id": "y5VTNmfm6fwX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from tqdm import tqdm,tqdm_pandas"
      ],
      "metadata": {
        "id": "7-9JfSsE7MsG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "from keras import applications,models, losses,optimizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.layers import Lambda, Flatten, Dense\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "jM3qgnmg72WG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "import cv2\n",
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import cv2"
      ],
      "metadata": {
        "id": "7VlO3XlX8QlA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import applications\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D,LeakyReLU\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, Activation, Lambda, Permute, Reshape\n"
      ],
      "metadata": {
        "id": "ize-7ZNsHwND"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import scipy as sp\n",
        "from scipy.spatial import distance\n",
        "from PIL import Image\n",
        "from keras.preprocessing import image"
      ],
      "metadata": {
        "id": "WJdchjcbH1Nn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "from hyperas.distributions import choice, uniform\n",
        "import hyperopt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import Sequence, to_categorical\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import *"
      ],
      "metadata": {
        "id": "gfxwaLphH-Ft"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
        "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
        "from keras.layers import (Convolution1D, Dense, Dropout, GlobalAveragePooling1D,\n",
        "                          GlobalMaxPool1D, Input, MaxPool1D, concatenate)\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "from os import listdir"
      ],
      "metadata": {
        "id": "NsIbQqb9IAS_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import dump\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding,LSTM,LSTMCell\n",
        "from tensorflow.keras.layers import add\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from keras.models import load_model\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from __future__ import print_function, division\n",
        "from builtins import range, input\n",
        "from keras.preprocessing import image   # for preprocessing the images\n",
        "from __future__ import print_function, division\n",
        "from builtins import range,input\n",
        "from datetime import datetime\n",
        "import scipy"
      ],
      "metadata": {
        "id": "oAnFU638IbYr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocess_img(p,shape = None):\n",
        "    Img = image.load_img(p, target_size=shape)\n",
        "    X = image.img_to_array(Img)\n",
        "    X = np.expand_dims(X,axis=0)\n",
        "    X = preprocess_input(X)\n",
        "    return X\n",
        "\n",
        "def preprocess_img(frame,shape = None):\n",
        "    X = np.expand_dims(frame,axis=0)\n",
        "    X = preprocess_input(X.astype(('float64')))\n",
        "    return X\n",
        "\n",
        "\n",
        "#Loading style image\n",
        "\n",
        "style_img = load_preprocess_img(p = '/content/starry_night.jpg', shape=(224,224))\n",
        "batch_shape = style_img.shape\n",
        "shape = style_img.shape[1:]\n",
        "shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "303jGj8ZIdY-",
        "outputId": "27ff661d-5860-46a2-9679-66689089e88b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (224,224,3)\n",
        "\n",
        "#Content model define\n",
        "def vgg_avg_pooling(shape):\n",
        "    vgg = VGG16(input_shape=shape,weights='imagenet',include_top=False)\n",
        "    model = Sequential()\n",
        "    for layer in vgg.layers:\n",
        "        if layer.__class__ == MaxPooling2D:\n",
        "        # replace it with average pooling\n",
        "            model.add(AveragePooling2D())\n",
        "        else:\n",
        "            model.add(layer)\n",
        "    return model\n",
        "\n",
        "def vgg_cutoff(shape,num_conv):\n",
        "    if num_conv<1|num_conv>13:\n",
        "        print('Error layer must be with in [1,13]')\n",
        "    model = vgg_avg_pooling(shape)\n",
        "    new_model = Sequential()\n",
        "    n=0\n",
        "    for layer in model.layers:\n",
        "        new_model.add(layer)\n",
        "        if layer.__class__ == Conv2D:\n",
        "            n+=1\n",
        "        if n >= num_conv:\n",
        "            break\n",
        "    return new_model\n",
        "\n",
        "#Style loss comutation graph\n",
        "\n",
        "def gram_matrix(img):\n",
        "    # input is (H, W, C) (C = # feature maps)\n",
        "    # we first need to convert it to (C, H*W)\n",
        "    X = K.batch_flatten(K.permute_dimensions(img,(2,0,1)))\n",
        "    # now, calculate the gram matrix\n",
        "    # gram = XX^T / N\n",
        "    gram_mat = K.dot(X,K.transpose(X))/img.get_shape().num_elements()\n",
        "    return gram_mat\n",
        "\n",
        "def style_loss(y,t):\n",
        "    return K.mean(K.square(gram_matrix(y)-gram_matrix(t)))\n",
        "\n",
        "def unpreprocess(img):\n",
        "    img[..., 0] += 103.939\n",
        "    img[..., 1] += 116.779\n",
        "    img[..., 2] += 126.68\n",
        "    img = img[..., ::-1]\n",
        "    return img\n",
        "\n",
        "def scale(x):\n",
        "    x = x-x.min()\n",
        "    x=x/x.max()\n",
        "    return x"
      ],
      "metadata": {
        "id": "UutKDTVAIiut"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading,processing and defining multi_output_model and style loss computation of style image\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "#Style image load and VGG model load.\n",
        "path = '/content/starry_night.jpg'\n",
        "img = image.load_img(path)\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x,axis=0)\n",
        "x = preprocess_input(x)\n",
        "    #shape\n",
        "batch_shape = x.shape\n",
        "shape = x.shape[1:]\n",
        "vgg = vgg_avg_pooling(shape)\n",
        "\n",
        "#Define multi-output model\n",
        "symb_conv_outputs = [layer.get_output_at(1) for layer in vgg.layers if layer.name.endswith('conv1')]\n",
        "multi_output_model = Model(vgg.input, symb_conv_outputs)\n",
        "symb_layer_out = [K.variable(y) for y in multi_output_model.predict(x)]\n",
        "\n",
        "#Conv layer weight matrix\n",
        "weights = [0.2,0.4,0.3,0.5,0.2]\n",
        "loss=0\n",
        "#Total style loss\n",
        "for symb,actual,w in zip(symb_conv_outputs,symb_layer_out,weights):\n",
        "    loss += w * style_loss(symb[0],actual[0])\n",
        "\n",
        "#gradients which are needed by the optimizer\n",
        "grad = K.gradients(loss,multi_output_model.input)\n",
        "get_loss_grad = K.function(inputs=[multi_output_model.input], outputs=[loss] + grad)\n",
        "\n",
        "#Scipy's minimizer function(fmin_l_bfgs_b) allows us to pass back function value f(x) and\n",
        "#its gradient f'(x), which we calculated in earlier step.\n",
        "#However, we need to unroll the input to minimizer function in1-D array format and both loss and gradient must be np.float64.\n",
        "\n",
        "def get_loss_grad_wrapper(x_vec):\n",
        "    l,g = get_loss_grad([x_vec.reshape(*batch_shape)])\n",
        "    return l.astype(np.float64), g.flatten().astype(np.float64)"
      ],
      "metadata": {
        "id": "jvYdIlBPJzFJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to minimize loss\n",
        "def min_loss(fn,epochs,batch_shape):\n",
        "    t0 = datetime.now()\n",
        "    losses = []\n",
        "    x = np.random.randn(np.prod(batch_shape))\n",
        "    for i in range(epochs):\n",
        "        x, l, _ = scipy.optimize.fmin_l_bfgs_b(func=fn,x0=x,maxfun=20)\n",
        "    # bounds=[[-127, 127]]*len(x.flatten())\n",
        "    #x = np.clip(x, -127, 127)\n",
        "    # print(\"min:\", x.min(), \"max:\", x.max())\n",
        "        print(\"iter=%s, loss=%s\" % (i, l))\n",
        "        losses.append(l)\n",
        "    print(\"duration:\", datetime.now() - t0)\n",
        "    plt.plot(losses)\n",
        "    plt.show()\n",
        "\n",
        "    newimg = x.reshape(*batch_shape)\n",
        "    final_img = unpreprocess(newimg)\n",
        "    return final_img[0]"
      ],
      "metadata": {
        "id": "f5k1o10JJ6wp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CNYp7LoDQ17J"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_shape = style_img.shape\n",
        "shape = style_img.shape[1:]\n",
        "cap = cv2.VideoCapture('D:/data_science/vedio/out.avi')\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "# Check if camera opened successfully\n",
        "if (cap.isOpened() == False):\n",
        "    print(\"Unable to read camera feed\")\n",
        "\n",
        "# Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
        "# We convert the resolutions from float to integer.\n",
        "\n",
        "# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
        "out = cv2.VideoWriter('D:/data_science/vedio/outpy.avi',cv2.VideoWriter_fourcc(*'MJPG'),20.0,(224,224))\n",
        "count = 0\n",
        "while(True):\n",
        "    ret, frame = cap.read()\n",
        "    frame = cv2.resize(frame,(224,224))\n",
        "    X = preprocess_img(frame)\n",
        "    vgg = vgg_avg_pooling(shape=shape)\n",
        "    content_model = Model(vgg.input,vgg.layers[13].get_output_at(1))\n",
        "    content_target = content_model.predict(X)\n",
        "    symb_conv_outputs = [layer.get_output_at(1) for layer in vgg.layers if layer.name.endswith('conv1')]\n",
        "    multi_output_model = Model(vgg.input, symb_conv_outputs)\n",
        "    symb_layer_out = [K.variable(y) for y in multi_output_model.predict(style_img)]\n",
        "    weights = [0.2,0.4,0.3,0.5,0.2]\n",
        "\n",
        "    loss=K.mean(K.square(content_model.output-content_target)) *4\n",
        "    for symb,actual,w in zip(symb_conv_outputs,symb_layer_out,weights):\n",
        "        loss += 0.03 * w * style_loss(symb[0],actual[0])\n",
        "\n",
        "    grad = K.gradients(loss,vgg.input)\n",
        "    get_loss_grad = K.Function(inputs=[vgg.input], outputs=[loss] + grad)\n",
        "    def get_loss_grad_wrapper(x_vec):\n",
        "        l,g = get_loss_grad([x_vec.reshape(*batch_shape)])\n",
        "        return l.astype(np.float64), g.flatten().astype(np.float64)\n",
        "    final_img = min_loss(fn=get_loss_grad_wrapper,epochs=30,batch_shape=batch_shape)\n",
        "    plt.imshow(scale(final_img))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    #cv2.imshow('output',frame)\n",
        "    #out.write(final_img)\n",
        "    filename =\"/content/starry_night.jpg\" % count;count+=1\n",
        "    cv2.imwrite(filename, final_img)\n",
        "    #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        #break\n",
        "\n",
        "    # Write the frame into the file 'output.avi'\n",
        "\n",
        "\n",
        "    # Display the resulting frame\n",
        "\n",
        "  # Break the loop\n",
        "    #if k == 27:\n",
        "    #    break\n",
        "\n",
        "# When everything done, release the video capture and video write objects\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindow\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "kIj0G4FcQ4Ll",
        "outputId": "9604fb6e-743b-49fd-a38e-daa714d4f864"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unable to read camera feed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-9799f3474d94>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_avg_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - Can't parse 'dsize'. Expected sequence length 2, got 3\n>  - Can't parse 'dsize'. Expected sequence length 2, got 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZMzhfrdQ93i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}